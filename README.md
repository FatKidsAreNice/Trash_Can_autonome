# Trash_Can_autonome
KI-basierter Autonomer Verfolgungs-Roboter (AI Object Tracker)

Dieses Projekt implementiert ein autonomes Robotersystem, das in der Lage ist, spezifische Objekte (z. B. Flaschen, B√§lle) in Echtzeit zu erkennen und ihnen physikalisch zu folgen. Das System kombiniert Computer Vision (YOLO) auf einem Edge-Device (Nvidia Jetson) mit einer Mikrocontroller-basierten Motorsteuerung (Arduino).
üìã Inhaltsverzeichnis

    Systemarchitektur

    Hardware-Anforderungen

    Software-Module

    Algorithmische Details

    Installation & Nutzung

    Konfiguration

üèõ Systemarchitektur

Das System folgt einem klassischen Sense-Think-Act-Paradigma, verteilt auf zwei Hardware-Ebenen:

    High-Level Computing (Nvidia Jetson):

        Wahrnehmung: Bildaufnahme und Objekterkennung mittels Deep Learning (YOLOv8/11).

        Logik: Objekt-Tracking (Identit√§tserhaltung) und Bewegungsplanung.

        Kommunikation: Sendet Steuerbefehle (Gas, Lenkung) via USB (Serial) an den Arduino.

    Low-Level Control (Arduino):

        Empf√§ngt Steuervektoren im Format <THROTTLE, STEERING>.

        Wandelt Vektoren in PWM-Signale f√ºr die Motortreiber um.

üõ† Hardware-Anforderungen

    Recheneinheit: Nvidia Jetson Nano / Orin Nano (f√ºr GPU-beschleunigte Inferenz).

    Kamera: Arducam IMX219 oder vergleichbare CSI/USB-Kamera mit Fokus-Steuerung.

    Mikrocontroller: Arduino Uno/Nano (verbunden via USB /dev/ttyACM0).

    Aktorik: Roboter-Chassis mit DC-Motoren und Motortreiber.

üìÇ Software-Module

Das Projekt ist modular aufgebaut, um Wartbarkeit und Austauschbarkeit zu gew√§hrleisten:
Datei	Beschreibung
main.py	Einstiegspunkt. Initialisiert Hardware, startet den Main-Loop und synchronisiert Vision und Aktorik.
yolo_detector.py	Perzeption. Wrapper f√ºr das YOLO-Modell. Filtert Ergebnisse nach Klasse, Konfidenz und Randbereich.
tracker_logic.py	Ged√§chtnis. Implementiert Centroid Tracking. Ordnet neuen Detektionen IDs zu und speichert verlorene Objekte kurzzeitig in einer History.
robot_brain.py	Regelung. Berechnet Lenkwinkel (P-Regler) und Geschwindigkeit basierend auf der Position und Gr√∂√üe des Zielobjekts.
config.py	Konfiguration. Zentrale Datei f√ºr Konstanten, Pfade und Tuning-Parameter.
gui.py	Visualisierung. Zeichnet Bounding Boxes, Status-Infos und die "Exit Zone" zur Fehleranalyse in das Videobild.
motor_test.py	Diagnose. Standalone-Skript zum Testen der seriellen Verbindung und der Motoren.
üß† Algorithmische Details
1. Objekterkennung & Filterung

Das System nutzt ein vortrainiertes YOLO-Modell (bevorzugt .engine Format f√ºr TensorRT).

    Semantischer Filter: Nur relevante Klassen (z. B. "bottle", "sports ball") werden akzeptiert.

    Geometrischer Filter: Objekte, die zu klein sind oder den Bildrand ber√ºhren, werden ignoriert, um Fehlsteuerungen zu vermeiden.

2. Zeitliche Koh√§renz (Tracking)

Da YOLO keine IDs liefert, nutzt tracker_logic.py einen euklidischen Distanz-Algorithmus:

    Matching: Verkn√ºpft die Detektion in Frame t mit dem Objekt in Frame t‚àí1, wenn die Distanz < MAX_TRACKING_DISTANCE ist.

    Objekt-Permanenz: Verschwindet ein Objekt (Verdeckung), wird es f√ºr HISTORY_DURATION (z.B. 5s) im Speicher gehalten ("Ghost"-Objekt). Taucht es an √§hnlicher Stelle wieder auf, erh√§lt es seine alte ID zur√ºck.

3. Bewegungssteuerung

Der RobotBrain nutzt einen einfachen Regelkreis:

    Lenkung: Proportional zur Abweichung der Objektmitte von der Bildmitte (error_x).

    Gas: Abh√§ngig von der Objektgr√∂√üe (Distanz). Ist das Objekt kleiner als TARGET_WIDTH_RATIO (40% Bildbreite), n√§hert sich der Roboter an.

üöÄ Installation & Nutzung
Voraussetzungen

    Python 3.8+

    Installierte Bibliotheken:
    Bash

    pip install ultralytics opencv-python pyserial numpy

    Ein exportiertes YOLO-Modell im Projektordner (definiert in config.py).

Starten des Systems

    Hardware-Test: Stellen Sie sicher, dass der Arduino verbunden ist.
    Bash

    python3 motor_test.py

    Hauptprogramm: Starten Sie den Tracker mit Angabe des I2C-Bus f√ºr den Fokus (meist 6, 7 oder 8 auf Jetson).
    Bash

    python3 main.py -i 7

Steuerung

    q: Programm beenden.

    f: Autofokus manuell triggern.

‚öôÔ∏è Konfiguration (config.py)

Wichtige Tuning-Parameter f√ºr Anpassungen an die Umgebung:

    SCALE_FACTOR: Skalierung des Eingangsbildes (z.B. 0.8) f√ºr schnellere Inferenz.

    CONFIDENCE_THRESHOLD: Ab welcher Sicherheit (0.0 - 1.0) ein Objekt erkannt wird.

    MEMORY_TOLERANCE: Wie viele Frames ein Objekt fehlen darf, bevor es in den "Lost"-Status √ºbergeht.

    MODEL_PATH: Pfad zur Modelldatei (.pt oder .engine).
